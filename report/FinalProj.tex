\documentclass{article}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\begin{document}

%------------------------Title page---------------------------
\title{Final Project - Comprehensive Classifier Creation}
\author{Sam Neyhart\\ Jon Clark Freeman\\ Kevin Sayarath\\ ECE 471 - Professor Qi}
\date{April 23, 2017}
\maketitle
\newpage

%------------------------Abstract-----------------------------
\begin{abstract}
The objective of this final project was to integrate various classification
techniques to achieve the best performance on the chosen dataset. This project
was a group effort where we explored both supervised and unsupervised
classificaiton techniques including MPP (all 3 cases), kNN (with different k's),
BPNN (developed), decision tree, and k-means, winner-take-all, and kohonen maps.
We also explored two classifier fusion techniques including majority vote fusion
and naive Bayes.
\end{abstract}
\newpage


%-----------------------Introduction---------------------------
\section*{Introduction}
\paragraph{}
This project performed for ECE 471 is an exercise using all the classificaiton methods
we have used in past projects with the additional exploration of two new fusion methods
which we previously had no experience with.  The dataset we are using is titled "Poker
Hand Dataset," publised by Robert Cattrel et. al., released in January 2007 *link to
dataset*.  By experimenting with various classification techniques, we learn the best
specific methodology to classify this particular dataset.
\paragraph{}
Overall the project served its purpose and was an opportunity to review and become experts
with the classification methods learned during this course.  We are proud to announce we
did not use any supporting libraries that do heavy-lifting of core computations, i.e. all
classification methods were re-written by team members for this project.

%\paragraph{} 
%Overall the project served its purpose and was an opportunity to learn
%much about the concepts of this course. For this lab the 
%numpy library was very helpful.

\paragraph{} 
We used a github repository to collaborate on this project:
\newline
https://github.com/sneyhart/swagteam6
\newpage


%-----------------------Team Member Contributions---------------------------
\section*{Team Member Contributions}
\subsection*{Clark}
\newline
\indent Mpp
\newline Knn
\newline BPNN
\newline Decision Tree
\newline Report Writing
\newline
\subsection*{Sam}
\newline
\indent Winner-take-all
\newline Kmeans
\newline Kohonen Map
\newline Report writing
\subsection*{Kevin}
\newline
\indent Normalization
\newline Evaluation
\newline Team organization
\newline Report writing
\newpage


%-----------------------Dataset----------------------
\section*{Dataset}
\paragraph{}
The dataset we chose consists of all possible permutations of five-card poker hands dealt
from a standard deck of 52 cards.  Each sample is one hand and each card is represented
by two features (suit and rank), for a total of 10 predictive attributes plus one class
attribute.  As previously mentioned, the order of cards is unimportant to its classification,
which is why there
are 480 possible Royal Flush hands as compared to only 4 in the combination set.
\subsection*{From poker-hand.names} 
Attribute Information:
\newline
   1) S1 ìSuit of card 1
\newline
\indent Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs}
\newline
   2) C1 ìRank of card 1
\newline
\indent Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King)
\newline
   3) S2 ìSuit of card 2
\newline
\indent Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs}
\newline
   4) C2 ìRank of card 2
\newline
\indent Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King)
\newline
   5) S3 ìSuit of card #3
\newline
\indent Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs}
\newline
   6) C3 ìRank of card #3
\newline
\indent Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King)
\newline
   7) S4 ìSuit of card #4
\newline
\indent Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs}
\newline
   8) C4 ìRank of card #4
\newline
\indent Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King)
\newline
   9) S5 ìSuit of card #5
\newline
\indent Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs}
\newline
   10) C5 ìRank of card #5
\newline
\indent Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King)
\newline
   11) CLASS ìPoker Handî
\newline
\indent Ordinal (0-9)
\newline
\newline
Class Information:
\newline
      0: Nothing in hand; not a recognized poker hand 
\newline
      1: One pair; one pair of equal ranks within five cards
\newline
      2: Two pairs; two pairs of equal ranks within five cards
\newline
      3: Three of a kind; three equal ranks within five cards
\newline
      4: Straight; five cards, sequentially ranked with no gaps
\newline
      5: Flush; five cards with the same suit
\newline
      6: Full house; pair + different rank three of a kind
\newline
      7: Four of a kind; four equal ranks within five cards
\newline
      8: Straight flush; straight + flush
\newline
      9: Royal flush; {Ace, King, Queen, Jack, Ten} + flush
\newline
\newline
N: 25,010 training, 1,000,000 testing
\newpage


%-----------------------Technical Approach----------------------
\section*{Technical Approach}
\paragraph{MPP} 
The mpp.py Python script implements the MPP algorithm using basic Python, the numpy library,
and the matplotlib library.  It performs MPP parametric-based classification by first
calculating the mean and covariance matricies from the dataset.
*Bayes/Discriminant Func*
\paragraph{Case 1}
The features are statistically independent, and have the same variance.  
Geometrically, the samples fall in equal-size hyperspherical clusters.  
Decision boundary: hyperplane of d-1 dimension.  This classification technique employs the
linear discriminant function and linear machine.  Additionally, when prior probabilities are
the same, the discriminant function is actually measuring the minimum distance from each
feature to each of the c mean vectors.
\paragraph{Case 2}
The covariance matrices for all the classes are identical but not a scalar of identity matrix.
Geometrically, the samples fall in hyperellipsoidal clusters.  
Decision boundary: hyperplane of d-1 dimension.
\paragraph{Case 3}
No assumption: the covariance matrices are different for each class.  
Quadratic classifier.  
Decision boundary: hyperquadratic for 2-D Gaussian.

\paragraph{kNN} 
The knn.py Python script implements the kNN algorithm using basic Python, the numpy library,
and the scipy library.  It performs kNN classification, or majority voting, using Euclidean
distance to assign a random sample according to the majority representation of classes
within the enclosing hypersphere of k nearest neighbors.  We experiment with different k's
and evaluate performance.

\paragraph{BPNN} 
The backprop.py script implements the backpropogation algorithm using basic Python, the numpy
library, and the random library.  Backpropogation is a type of multilayer feedforward network
that calculates the difference between unit output and expected output, "back-propogating"
this delta from the output layers back toward the feeding layers.  This technique causes the
network to "learn" correct classifictions in a supervised architecture. The script uses a network with 2 hidden layes. Each hidden layer has 15 neurons. The input layers change based on if the PCA preprocessing has been done on the input data. The output for the network is one neuron for the classification.

\paragraph{Decision Tree} 
The dtree.py script implements the decision tree architecture using basic Python and the numpy
library.  The decision tree is structured to decide the various classes in an increasing faction. The highest valued
class that is decided is the resulting classification of that hand. This is very efficient because the features of the
data set are such that each class is made up of a specific set of cards. Thus each node will test the hand and classify it into that hand.

\paragraph{Kmeans} 
The \textit{kmeans} program was written for this project in C because we believed
that python would not be able to perform the kmeans algorithm within an acceptable 
time span. The files \textit{hand.c} and \textit{hand.h} contain the definition of
a hand struct as well some useful functions relating to this struct. Our kmeans 
program makes use of these two file in order to perform kmeans clustering on our data
set. Despite having been written in C, the algorithm can take several minutes large data 
sets. Because of the the nature of poker hands, clustering on the raw data does not 
perform particularily well.

\paragraph{Winner-Take-All}
In order to perform winner-take-all clustering on our data set the program \textit{wta} was
written in C. The general structure of this program is the same as that of the kmeans program.
The file input and output sections of code are reused and \textit{wta} also makes use of
the hand.c and hand.h files. The main way in which our winner-take-all program differs from
our kmeans program is in the loop where the actual algorith is performed. This program is 
very fast (less than 5 seconds) even for our largest data set (1,000,000 data points). 

\paragraph{Kohonen Maps}
In order to the perform the kohonen map algorithm on our data set the program \textit{kohonen} was
written in C. The general structure of this program is the same as that of the kmeans and
winner-take-all programs. The file input and output sections of code are reused and 
\textit{kohonen} also makes use of of the hand.c and hand.h files. Once again all major
changes were made in the main loop where the algorithm is performed. The speed of this program
is comporable to \textit{wta}.


%---------------------Experiments and Results-------------------------
\newpage
\section*{Experiments and Results}
\paragraph{MPP}
\centerline{\includegraphics[width = 4.5in]{images/mpp-results}}

\paragraph{kNN}
\centerline{\includegraphics[width = 4.5in]{images/knn-results}}

\paragraph{BPNN}
\centerline{\includegraphics[width = 4.5in]{images/bpnn-results}}

\paragraph{Decision Tree}
The Decision tree algorithm was actually able to classify with an accuracy of one hundred percent when run on the testing set. The algorithm simply ran through each of the hands and had a node that decided for each of the groups since each group is defined in such a way that is known before hand and has to be a certain combination of the cards. This lead the algorithm to be very fast as well since it only goes through the data set one time to classify each hand.

\paragraph{K-means}
\centerline{\includegraphics[width = 4.5in]{images/KmeansAccuracyRaw}}

\paragraph{Winner-Take-All}
\centerline{\includegraphics[width = 4.5in]{images/WtaAccuracyRaw}}

\paragraph{Kohonen Maps}
\centerline{\includegraphics[width = 4.5in]{images/KohonenAccuracyRaw}}
\newpage


%-----------------------------Evaluation------------------------
\section*{Evaluation}
\paragraph{}
Evalution was performed using 10-fold cross-validation for the kNN classifier.
Evaluation is performed in evaluation.py using standard Python language, with the support of
the math, random, numpy, and subprocess libraries.  This script performes the m-fold
cross-validation algorithm by dividing the dataset into m disjoint sets and iterates over them
using the 'leave-one-out' methodology, where one set is preserved as the testing (or validaiton)
set and the remaining m-1 sets are combined to become the new training set.  Normalization is
performed anew on each of the m iterations for the training and testing sets separately.
\newline
\newline
\centerline{\includegraphics[width = 4.5in]{images/KNN_MFOLD2.png}}
\newline
\newline
kNN with 10-fold validation has a similar profile to the previous kNN trials performed with
generally lower accuracy due to near complete mitigation of training/testing cycle bias.
\newpage


%-----------------------------Discussion------------------------
\section*{Discussion}
\paragraph{}
This project was very interesting to perform. Employing all of the classifiers that the course has covered
proved to be a difficult yet enjoyable exercise. Using python was a good choice for most of the programs 
because of the simplicity of programming in Python as well as the ever useful libraries provided in the language.
The clustering programs were written in c because of the size of the data set. C performed much faster than Python would have
thus the group was able to run more tesst to glean a reasonable understanding of how well the programs performed. This was a very
rewarding project to work on for the team. 
\newpage


%----------------------------Appendix-----------------------------
\appendix
\section*{Appendix}
\subsection*{normalize.py}
\lstinputlisting[language=Python,numbers=left]{../normalize.py}
\newpage
\subsection*{mpp.py}
\lstinputlisting[language=Python,numbers=left]{../mpp.py}
\newpage
\subsection*{knn.py}
\lstinputlisting[language=Python,numbers=left]{../knn.py}
\newpage
\subsection*{backprop.py}
\lstinputlisting[language=Python,numbers=left]{../backprop.py}
\newpage
\subsection*{dtree.py}
\verbatiminput{../dtree.py}
\newpage
\subsection*{kmeans.c}
\lstinputlisting[language=C,numbers=left]{../kmeans.c}
\newpage
\subsection*{wta.c}
\lstinputlisting[language=C,numbers=left]{../wta.c}
\newpage
\subsection*{kohonen.c}
\lstinputlisting[language=C,numbers=left]{../kohonen.c}
\newpage
\subsection*{hand.c}
\lstinputlisting[language=C,numbers=left]{../hand.c}
\newpage
\subsection*{hand.h}
\lstinputlisting[language=C,numbers=left]{../hand.h}
\newpage
\subsection*{evaluation.py}
\lstinputlisting[language=Python,numbers=left]{../evaluation.py}
\newpage

\end{document}
